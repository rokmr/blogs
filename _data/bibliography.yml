# Global Bibliography
# Citations can be referenced in posts using [@key] syntax
# Format follows BibTeX-like structure

vaswani2017attention:
  type: article
  authors: "Vaswani, A., Shazeer, N., Parmar, N., et al."
  title: "Attention Is All You Need"
  venue: "NeurIPS 2017"
  year: 2017
  url: "https://arxiv.org/abs/1706.03762"
  doi: "10.48550/arXiv.1706.03762"

devlin2019bert:
  type: article
  authors: "Devlin, J., Chang, M., Lee, K., Toutanova, K."
  title: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
  venue: "NAACL 2019"
  year: 2019
  url: "https://arxiv.org/abs/1810.04805"

dao2022flashattention:
  type: article
  authors: "Dao, T., Fu, D., Ermon, S., Rudra, A., RÃ©, C."
  title: "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"
  venue: "NeurIPS 2022"
  year: 2022
  url: "https://arxiv.org/abs/2205.14135"

brown2020gpt3:
  type: article
  authors: "Brown, T., Mann, B., Ryder, N., et al."
  title: "Language Models are Few-Shot Learners"
  venue: "NeurIPS 2020"
  year: 2020
  url: "https://arxiv.org/abs/2005.14165"

ho2020ddpm:
  type: article
  authors: "Ho, J., Jain, A., Abbeel, P."
  title: "Denoising Diffusion Probabilistic Models"
  venue: "NeurIPS 2020"
  year: 2020
  url: "https://arxiv.org/abs/2006.11239"

dosovitskiy2021vit:
  type: article
  authors: "Dosovitskiy, A., Beyer, L., Kolesnikov, A., et al."
  title: "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
  venue: "ICLR 2021"
  year: 2021
  url: "https://arxiv.org/abs/2010.11929"
